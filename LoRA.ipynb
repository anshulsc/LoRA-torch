{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Value Decomposition : Idea behind LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0797,  0.5545,  0.8058, -0.7140, -0.1518,  1.0773,  2.3690,  0.8486,\n",
      "         -1.1825, -3.2632],\n",
      "        [-0.3303,  0.2283,  0.4145, -0.1924, -0.0215,  0.3276,  0.7926,  0.2233,\n",
      "         -0.3422, -0.9614],\n",
      "        [-0.5256,  0.9864,  2.4447, -0.0290,  0.2305,  0.5000,  1.9831, -0.0311,\n",
      "         -0.3369, -1.1376],\n",
      "        [ 0.7900, -1.1336, -2.6746,  0.1988, -0.1982, -0.7634, -2.5763, -0.1696,\n",
      "          0.6227,  1.9294],\n",
      "        [ 0.1258,  0.1458,  0.5090,  0.1768,  0.1071, -0.1327, -0.0323, -0.2294,\n",
      "          0.2079,  0.5128],\n",
      "        [ 0.7697,  0.0050,  0.5725,  0.6870,  0.2783, -0.7818, -1.2253, -0.8533,\n",
      "          0.9765,  2.5786],\n",
      "        [ 1.4157, -0.7814, -1.2121,  0.9120,  0.1760, -1.4108, -3.1692, -1.0791,\n",
      "          1.5325,  4.2447],\n",
      "        [-0.0119,  0.6050,  1.7245,  0.2584,  0.2528, -0.0086,  0.7198, -0.3620,\n",
      "          0.1865,  0.3410],\n",
      "        [ 1.0485, -0.6394, -1.0715,  0.6485,  0.1046, -1.0427, -2.4174, -0.7615,\n",
      "          1.1147,  3.1054],\n",
      "        [ 0.9088,  0.1936,  1.2136,  0.8946,  0.4084, -0.9295, -1.2294, -1.1239,\n",
      "          1.2155,  3.1628]])\n"
     ]
    }
   ],
   "source": [
    "d, k = 10, 10\n",
    "\n",
    "# This matrix is rank-deficient \n",
    "W_rank = 2\n",
    "W = torch.randn(d,W_rank) @ torch.randn(W_rank,k)\n",
    "print(W)\n",
    "# the matrix is 10 by 10 but the rank of this matrix is still 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the rank of matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of matrix is 2\n"
     ]
    }
   ],
   "source": [
    "W_rank = np.linalg.matrix_rank(W)\n",
    "print(f'Rank of matrix is {W_rank}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calaculate SVD decompostion of this matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_r is torch.Size([2, 2])\n",
      "V_r is torch.Size([2, 10])\n",
      "U_r is torch.Size([10, 2])\n",
      "Shape of B : torch.Size([10, 2])\n",
      "Shape of A : torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# Perform SVD\n",
    "U, S, V = torch.svd(W)\n",
    "\n",
    "# For rank-r factorization, kepp only first r singular values\n",
    "\n",
    "U_r = U[:,:W_rank]\n",
    "S_r = torch.diag(S[:W_rank])\n",
    "V_r = V[:,:W_rank].t()\n",
    "\n",
    "print(f\"S_r is {S_r.shape}\")\n",
    "print(f\"V_r is {V_r.shape}\")\n",
    "print(f\"U_r is {U_r.shape}\")\n",
    "\n",
    "\n",
    "B = U_r @ S_r\n",
    "A = V_r\n",
    "\n",
    "print(f'Shape of B : {B.shape}')\n",
    "print(f'Shape of A : {A.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Given the sampe input, check the output using the original W  matrix and the matrics resulting from the decompsotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y using W \n",
      " tensor([ 7.2684e+00,  2.3162e+00,  7.7151e+00, -1.0446e+01, -8.1639e-03,\n",
      "        -3.7270e+00, -1.1146e+01,  2.0207e+00, -9.6258e+00, -4.1163e+00])\n",
      "\n",
      "Computed y using BA \n",
      " tensor([ 7.2684e+00,  2.3162e+00,  7.7151e+00, -1.0446e+01, -8.1639e-03,\n",
      "        -3.7270e+00, -1.1146e+01,  2.0207e+00, -9.6258e+00, -4.1163e+00])\n"
     ]
    }
   ],
   "source": [
    "bias = torch.randn(d)\n",
    "x = torch.randn(d)\n",
    "\n",
    "# y = Wx + b\n",
    "y = W @ x + bias\n",
    "\n",
    "# y' = CRx = b\n",
    "y_prime = (B @ A)@ x + bias\n",
    "\n",
    "print(\"Original y using W \\n\", y)\n",
    "print()\n",
    "print(\"Computed y using BA \\n\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Params in W : 100\n",
      "Total Params in B and A : 40\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Params in W : {W.nelement()}\")\n",
    "print(f\"Total Params in B and A : {B.nelement() + A.nelement()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Impplementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:02<00:00, 3572544.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 5915695.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 1882655.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2545841.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RichBoyNet(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size_1 = 1000, hidden_size_2 = 2000):\n",
    "        super(RichBoyNet,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_size_1)\n",
    "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
    "        self.linear3 = nn.Linear(hidden_size_2,10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "net = RichBoyNet().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6000/6000 [00:58<00:00, 102.02it/s, loss=0.125]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, ne, epochs=5, total_iterations_limit=None):\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "\n",
    "        loss_sum = 0\n",
    "        num_iterations = 0\n",
    "\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x,y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(x.view(-1,28*28))\n",
    "            loss = cross_entropy(output,y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "    \n",
    "    print(x.shape)\n",
    "\n",
    "train(train_loader,net, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep copy of the original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = {}\n",
    "for name, param in net.named_parameters():\n",
    "    original_weights[name] = param.clone().detach()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear1.weight': tensor([[ 0.0491,  0.0412,  0.0896,  ...,  0.1083,  0.0500,  0.1037],\n",
       "         [ 0.0276,  0.0295,  0.0413,  ..., -0.0105, -0.0150, -0.0233],\n",
       "         [ 0.0405, -0.0032,  0.0142,  ...,  0.0140, -0.0163,  0.0253],\n",
       "         ...,\n",
       "         [ 0.0337,  0.0420,  0.0775,  ...,  0.0680,  0.0738,  0.0745],\n",
       "         [ 0.1268,  0.0771,  0.0724,  ...,  0.1025,  0.0667,  0.0914],\n",
       "         [ 0.0077,  0.0371,  0.0322,  ...,  0.0069, -0.0023,  0.0207]],\n",
       "        device='mps:0'),\n",
       " 'linear1.bias': tensor([-4.9370e-02, -1.7618e-02,  7.2536e-03, -4.5131e-03, -5.8461e-02,\n",
       "         -3.1394e-02, -9.1590e-02, -1.0883e-01, -1.0716e-01, -6.1761e-02,\n",
       "         -7.0232e-02, -1.5700e-02, -4.8010e-03, -9.8721e-02, -1.0534e-01,\n",
       "         -7.4751e-02, -5.3460e-02, -9.9413e-03, -1.2278e-02, -9.6957e-02,\n",
       "         -8.8563e-02, -2.4805e-02, -5.2471e-02, -4.4481e-02, -6.3462e-02,\n",
       "         -8.7155e-02, -9.0936e-02,  8.1895e-03, -4.9283e-02, -4.5271e-02,\n",
       "         -4.2645e-02,  1.4025e-02, -2.6896e-02, -1.5687e-02,  1.5095e-03,\n",
       "         -5.3956e-02, -2.7984e-02, -1.1426e-01,  2.9551e-03, -5.7772e-02,\n",
       "         -3.4866e-02, -1.2743e-02,  4.0973e-03, -7.3954e-02, -1.7695e-02,\n",
       "         -2.9106e-02, -1.5280e-02, -7.9701e-02, -5.3020e-02, -8.8079e-03,\n",
       "         -2.8771e-02, -8.0192e-02, -7.8738e-02, -4.4919e-02, -4.8068e-02,\n",
       "         -5.4562e-02, -8.4037e-02, -7.4352e-02, -1.7774e-02,  3.0318e-03,\n",
       "         -2.9790e-02, -4.6023e-02, -7.9446e-02, -1.2510e-01, -2.6097e-02,\n",
       "         -2.8764e-02, -2.2382e-02,  1.2442e-02, -5.1946e-03, -8.5105e-02,\n",
       "         -7.2705e-02, -3.6739e-02, -1.0741e-01,  1.0519e-02, -1.9165e-04,\n",
       "         -6.6779e-02, -1.3947e-02, -3.6324e-02, -2.4639e-02, -2.7649e-02,\n",
       "         -7.2883e-02, -3.6491e-02, -7.7737e-02,  4.7958e-03, -8.7450e-02,\n",
       "         -1.0695e-01, -2.8455e-02, -5.5428e-02, -8.3996e-02,  4.1941e-02,\n",
       "         -3.2466e-02, -3.3256e-02, -4.1976e-02, -3.2039e-02, -7.3127e-02,\n",
       "         -2.7926e-02, -3.6586e-02, -5.7129e-02,  5.3004e-03, -1.0505e-01,\n",
       "         -1.3619e-02, -7.9860e-02, -9.8835e-02,  3.2287e-03, -6.7742e-02,\n",
       "         -2.3274e-03, -6.9264e-02, -6.0082e-02, -1.2237e-01, -4.4027e-02,\n",
       "         -3.1702e-02, -2.0916e-02, -7.0073e-02, -4.4376e-02, -1.3532e-01,\n",
       "         -3.2925e-02, -3.0277e-02, -2.2786e-03, -4.1347e-02,  1.9381e-01,\n",
       "         -5.2508e-02, -3.6750e-02, -2.3656e-02, -5.2269e-02, -2.8576e-02,\n",
       "         -5.6923e-02, -4.0166e-02, -1.6679e-02, -4.5209e-02, -5.8799e-02,\n",
       "         -8.9909e-02, -2.5717e-02, -9.1633e-02, -5.9585e-02, -2.1901e-02,\n",
       "         -4.4202e-02, -4.0545e-02,  9.4564e-03, -4.9187e-02, -7.7670e-02,\n",
       "         -1.0810e-01, -4.4329e-02, -5.3464e-02, -4.4085e-02, -4.1411e-02,\n",
       "         -5.6074e-02, -6.0520e-02, -6.1493e-02, -1.6996e-02, -1.0449e-01,\n",
       "         -2.3736e-02, -2.7106e-02, -7.0781e-02, -8.4154e-02, -1.2508e-01,\n",
       "          8.5002e-03, -8.9852e-02, -9.2524e-03, -3.7717e-02, -5.4351e-02,\n",
       "         -8.2621e-02, -4.1935e-02, -6.1438e-02, -4.9641e-02, -7.4699e-02,\n",
       "         -3.0939e-02, -9.6514e-02, -1.1675e-02, -1.3782e-01, -3.1560e-02,\n",
       "         -2.2732e-02, -8.7743e-02, -4.5350e-02, -1.6549e-02, -8.4222e-02,\n",
       "         -9.4519e-02, -3.1569e-02, -7.4575e-02, -1.8313e-02,  2.7774e-03,\n",
       "         -1.1309e-01, -7.0329e-02, -1.6915e-02, -1.8551e-02, -7.4257e-02,\n",
       "         -3.2743e-02, -2.6720e-02, -5.3089e-02, -3.3374e-02, -6.9275e-02,\n",
       "          7.2702e-03,  4.3309e-03, -4.3423e-02, -3.0213e-02, -5.8199e-03,\n",
       "         -5.7968e-02, -2.0161e-02, -1.2274e-02, -5.7304e-02, -1.4323e-01,\n",
       "         -6.7816e-02, -5.9691e-02, -1.1308e-02, -2.4806e-02, -7.7719e-02,\n",
       "         -4.1352e-02, -3.8375e-02, -2.1788e-02,  7.2731e-03,  1.4398e-01,\n",
       "         -5.6283e-02, -9.8037e-02, -2.6001e-02, -3.4624e-02, -4.1957e-02,\n",
       "         -7.2472e-02, -5.1000e-02, -5.7233e-02, -1.1165e-02, -1.1747e-01,\n",
       "         -3.0784e-02, -9.4003e-03, -1.3223e-02, -8.4426e-02,  2.6135e-02,\n",
       "         -1.9949e-02, -4.0744e-02,  1.2903e-03, -2.8662e-03, -5.0889e-02,\n",
       "         -2.2962e-02,  2.4417e-02, -4.8903e-02, -6.4296e-02, -8.5573e-02,\n",
       "         -3.8506e-03, -4.1965e-02, -7.9800e-02, -5.7026e-02, -4.7816e-02,\n",
       "         -1.5708e-02, -4.1192e-02, -8.1692e-02, -1.4985e-02, -1.1755e-01,\n",
       "         -4.7034e-02, -5.5148e-02, -1.2335e-01, -2.5403e-02, -7.6225e-02,\n",
       "         -4.0684e-02, -5.4413e-02, -2.2165e-02,  1.6501e-02, -7.3665e-02,\n",
       "          2.0059e-03, -6.4737e-02, -4.5634e-02,  1.1225e-02, -3.2854e-02,\n",
       "         -8.7964e-02, -1.0403e-01, -3.7534e-02, -3.4559e-02, -5.8879e-02,\n",
       "         -1.3973e-02, -9.2418e-02, -8.0194e-02,  2.1809e-02, -7.2297e-02,\n",
       "         -4.4931e-03,  8.9468e-04, -9.1146e-02, -1.0501e-01, -2.2552e-02,\n",
       "         -4.9992e-02, -5.0105e-02, -4.2019e-02, -8.6506e-02, -9.0157e-02,\n",
       "         -1.5239e-01, -1.7365e-02, -6.6476e-03, -3.1346e-02,  7.2220e-04,\n",
       "         -3.9412e-02, -8.2507e-02,  2.1022e-04, -3.3010e-02, -6.1284e-02,\n",
       "          1.1943e-02, -6.5852e-02, -3.9117e-02, -5.9461e-02, -5.5884e-02,\n",
       "         -8.4539e-02, -2.7595e-02, -5.8604e-02, -2.6845e-02, -4.7870e-02,\n",
       "         -6.0717e-02, -9.2749e-02,  3.8815e-03, -1.0962e-01, -2.7371e-02,\n",
       "         -8.4664e-03, -2.6009e-02, -1.5071e-01, -4.0769e-02, -1.5091e-02,\n",
       "         -4.3495e-03, -4.2577e-02, -9.7425e-02, -5.9532e-02, -9.5901e-02,\n",
       "         -2.1540e-02, -3.1142e-02, -1.6248e-02,  1.2937e-02, -9.4536e-02,\n",
       "         -3.4944e-02, -9.7023e-02, -8.5014e-02, -2.5032e-02, -4.6759e-02,\n",
       "         -7.7129e-02, -1.3030e-02, -1.0878e-02, -4.7537e-02, -1.9890e-02,\n",
       "         -9.6099e-02, -1.6843e-02, -8.6772e-03, -5.3177e-02, -8.2356e-02,\n",
       "         -1.8269e-02, -1.9379e-02, -5.6920e-02, -3.4504e-02, -5.2177e-02,\n",
       "         -1.1744e-01,  2.0753e-02, -5.5317e-03, -2.6466e-03, -3.3002e-02,\n",
       "         -9.2501e-02, -6.6312e-02, -7.3000e-03, -5.8521e-02,  3.8099e-03,\n",
       "         -1.0341e-01, -2.0843e-02, -3.4318e-02, -9.5949e-02, -2.7892e-02,\n",
       "         -8.0287e-03, -3.1381e-02,  1.7549e-02, -3.9357e-02, -6.1441e-02,\n",
       "         -8.4441e-02, -2.8620e-02, -5.8057e-02, -3.9880e-02, -4.0577e-02,\n",
       "         -4.7112e-02, -5.0855e-02, -4.7619e-02,  1.5132e-02, -4.6201e-02,\n",
       "          6.6066e-03, -7.6103e-02, -1.2676e-02, -4.9629e-02, -1.3990e-02,\n",
       "         -6.1043e-02, -3.2228e-02, -7.2870e-02, -2.4267e-02,  1.7962e-02,\n",
       "         -1.2078e-02, -2.1832e-03, -1.4402e-02,  3.6036e-02, -2.0196e-02,\n",
       "         -8.7289e-02, -8.4273e-02, -3.9157e-02, -4.0086e-02, -5.6616e-02,\n",
       "         -4.7377e-02, -3.8220e-02, -1.8094e-02, -1.7862e-02, -3.7892e-02,\n",
       "         -6.5223e-02, -1.7308e-02, -5.1829e-02, -2.6777e-02, -3.1596e-02,\n",
       "         -6.8763e-02, -4.3615e-02, -8.8657e-03, -4.2015e-02,  4.0617e-04,\n",
       "         -5.3775e-02, -5.4941e-02, -6.4368e-02, -5.4052e-02, -7.5545e-02,\n",
       "         -3.6495e-02,  3.2576e-02, -5.2593e-02, -6.5232e-02, -2.4000e-02,\n",
       "         -2.6774e-02, -3.1237e-02,  1.4177e-02, -6.7810e-02,  3.4820e-03,\n",
       "         -1.3597e-01, -1.1680e-01, -8.3957e-02, -5.9517e-02, -4.7995e-02,\n",
       "         -4.9948e-02, -1.8783e-02, -4.1459e-02, -1.7724e-02, -6.5901e-02,\n",
       "         -4.6148e-02, -2.4357e-03, -6.6855e-02, -5.4979e-02, -9.8199e-02,\n",
       "         -6.3388e-02, -6.3802e-02, -6.0025e-02, -8.7501e-02, -7.8779e-02,\n",
       "         -9.2899e-02, -2.2443e-02, -3.3746e-02, -2.0676e-02, -5.6430e-03,\n",
       "         -4.0207e-02, -4.1769e-04, -1.5854e-02, -1.0165e-01, -2.8643e-02,\n",
       "         -7.8644e-03, -7.0454e-02, -1.0548e-03, -4.3984e-02, -4.3771e-02,\n",
       "         -6.2485e-02, -4.9409e-02, -3.2857e-02, -1.2737e-02, -6.2404e-03,\n",
       "          2.3078e-02, -1.8986e-02, -4.8848e-03,  4.5057e-02, -1.1446e-01,\n",
       "         -4.0647e-02, -6.6586e-02, -2.6788e-02, -8.4633e-02, -4.9445e-02,\n",
       "         -2.9842e-03, -6.5530e-02, -6.7600e-02, -2.8564e-02,  3.9166e-03,\n",
       "         -2.7310e-02, -6.0974e-02, -7.5733e-02,  3.9223e-03, -3.8297e-02,\n",
       "         -2.0303e-02, -8.7668e-02, -1.1755e-01, -8.5904e-02, -3.4555e-02,\n",
       "         -6.8805e-02, -1.9730e-02, -6.5338e-02, -2.5264e-02, -7.3488e-02,\n",
       "         -2.3329e-02, -9.4443e-02, -8.8820e-02, -8.7589e-03, -2.7725e-02,\n",
       "         -3.6478e-02, -8.8505e-02, -1.6632e-02,  2.5618e-02, -7.5546e-02,\n",
       "          4.7118e-03,  1.4201e-02, -3.6690e-02, -5.2626e-02, -5.5246e-02,\n",
       "         -3.9788e-02, -8.1459e-03, -2.2124e-02, -3.1396e-02, -1.8876e-02,\n",
       "         -1.5122e-02, -5.9147e-02, -7.5464e-02, -7.5614e-02,  7.2501e-03,\n",
       "         -1.1278e-02, -9.9013e-02, -4.9035e-02, -9.7308e-02,  3.7411e-03,\n",
       "         -5.7370e-02, -6.1649e-02, -8.5837e-02, -1.8683e-02, -9.2230e-02,\n",
       "         -4.7541e-02, -4.4416e-02,  1.4151e-02, -3.0546e-02, -3.0795e-02,\n",
       "         -1.0486e-01,  1.6361e-02, -2.6594e-02, -4.8674e-04, -1.1915e-01,\n",
       "         -2.5585e-02, -1.8925e-02, -4.8340e-02,  1.3744e-02, -5.4299e-02,\n",
       "          7.1232e-03, -3.6850e-02, -4.8858e-02, -5.2592e-02, -9.9149e-03,\n",
       "         -4.2360e-02, -7.1173e-02, -9.0630e-02, -1.0371e-01, -2.6710e-03,\n",
       "         -6.7482e-02, -5.5428e-02, -1.3043e-02, -2.8816e-02, -3.8999e-02,\n",
       "         -2.0247e-01, -1.7801e-02, -3.1450e-02, -1.0214e-01, -4.5739e-02,\n",
       "         -1.3179e-02, -5.1676e-02, -9.9422e-02, -7.0478e-02, -5.3080e-02,\n",
       "         -1.0672e-01,  1.3585e-02, -3.6584e-02, -1.5491e-02, -6.2363e-02,\n",
       "          8.1648e-03, -7.0236e-02, -3.3851e-02,  9.6958e-03, -7.6809e-03,\n",
       "         -6.1677e-02, -3.2836e-02, -5.7986e-03, -1.7003e-02, -4.8274e-02,\n",
       "         -2.6836e-02, -1.4187e-01, -8.3101e-02, -3.3542e-02, -4.1561e-02,\n",
       "         -1.0915e-01, -2.9689e-02, -4.0448e-02,  1.6316e-02, -7.9099e-02,\n",
       "         -6.7915e-02, -3.2900e-02, -6.1971e-02, -6.1021e-02, -6.5688e-02,\n",
       "         -4.8232e-03,  9.3568e-03, -9.8184e-02, -3.6707e-02, -3.1134e-02,\n",
       "         -2.8295e-02, -5.0325e-02, -1.1248e-02, -5.1890e-03, -2.1153e-02,\n",
       "         -1.7000e-02, -8.3397e-03, -7.5011e-02, -9.0794e-02, -1.7769e-02,\n",
       "         -3.5702e-03, -4.2862e-02, -8.9499e-02, -1.5137e-02, -6.1142e-03,\n",
       "          7.4632e-03, -7.4513e-02, -9.4731e-02,  5.9224e-03, -4.6089e-02,\n",
       "         -3.6883e-02, -3.1149e-02, -8.2393e-03, -7.4196e-02, -7.3169e-02,\n",
       "          7.1527e-03, -3.7139e-02, -5.2766e-02, -1.1309e-02, -1.3372e-02,\n",
       "         -3.8685e-02, -1.5990e-03, -8.2105e-02, -5.4911e-02, -7.2551e-02,\n",
       "         -4.0344e-02, -4.2603e-02, -1.0823e-01, -3.1177e-02, -3.4267e-02,\n",
       "         -2.3052e-03,  9.2584e-03, -1.2480e-02,  1.5862e-02, -4.0507e-02,\n",
       "         -8.3669e-03, -1.6704e-02, -8.0799e-02, -8.7362e-03, -9.0183e-02,\n",
       "         -4.4604e-02, -5.4661e-02, -1.2096e-02, -4.4626e-02, -6.2515e-02,\n",
       "         -8.1513e-02, -3.3842e-02,  1.4909e-02, -6.6167e-02, -8.0742e-02,\n",
       "         -3.3310e-02, -3.3105e-02, -2.4101e-02, -5.5979e-02, -9.1485e-03,\n",
       "         -9.7278e-02,  1.2284e-02, -2.9631e-02, -3.3781e-02, -4.0384e-02,\n",
       "         -2.7945e-03, -4.1639e-02, -5.9356e-02, -4.3615e-02, -3.1368e-02,\n",
       "         -3.2393e-02, -4.9241e-02, -3.3776e-02, -3.9631e-02, -2.2803e-02,\n",
       "         -8.4211e-02, -5.0846e-02, -5.2000e-04, -8.7615e-02, -1.2876e-02,\n",
       "         -6.5660e-02, -6.0050e-02, -5.0121e-02, -7.1841e-02, -5.9205e-02,\n",
       "         -1.2415e-02, -3.7465e-02, -3.5774e-02, -5.6281e-02, -1.8170e-02,\n",
       "         -5.7882e-02,  1.1020e-03, -3.3989e-02, -5.4321e-02, -5.7049e-02,\n",
       "         -7.1002e-04, -7.2678e-02, -8.5647e-02, -9.2096e-02, -6.4422e-02,\n",
       "         -2.4048e-02, -4.2357e-02, -8.6034e-03, -1.7622e-02, -9.6863e-02,\n",
       "         -2.9988e-02, -3.0957e-02, -1.8157e-02, -3.9435e-02, -1.2574e-02,\n",
       "         -1.9276e-02, -2.9854e-02, -4.3882e-02, -1.4580e-01, -4.1308e-02,\n",
       "         -3.8425e-02, -5.1768e-02, -6.6304e-02, -6.3494e-02, -3.3728e-02,\n",
       "         -2.0055e-02, -2.5779e-02, -3.8043e-02, -2.8492e-02, -6.7212e-02,\n",
       "         -9.6543e-02, -4.5654e-03, -6.2349e-02, -3.9356e-03, -4.9615e-02,\n",
       "         -3.2697e-02,  4.3119e-03, -6.5687e-03, -1.9545e-02, -5.1856e-02,\n",
       "         -5.3594e-02, -6.9172e-02, -9.7239e-02,  7.0294e-03, -3.6702e-02,\n",
       "         -1.6441e-02, -2.3568e-02, -6.7024e-02, -6.2498e-02, -1.7609e-02,\n",
       "         -1.2785e-01, -1.8463e-02, -1.1861e-02, -6.1330e-02, -7.3654e-03,\n",
       "         -4.5002e-02, -2.5694e-02, -4.3728e-02, -8.7488e-03, -4.7947e-02,\n",
       "         -6.7444e-02, -5.5610e-02, -4.1991e-02, -1.2332e-02, -2.7686e-02,\n",
       "         -5.8717e-02, -8.6774e-02,  2.0700e-02, -3.6867e-02, -4.7008e-02,\n",
       "         -2.3631e-02, -5.0996e-02,  8.0460e-02, -1.0667e-02, -5.4548e-02,\n",
       "         -5.9248e-02, -2.9662e-02, -4.1706e-02, -3.3720e-02, -8.0172e-02,\n",
       "         -1.0794e-02, -5.2730e-02, -2.6247e-02, -1.0974e-01, -5.5256e-02,\n",
       "         -4.1905e-02, -9.0193e-04, -2.2670e-02, -2.2108e-02,  1.8002e-02,\n",
       "         -1.6398e-02, -3.3772e-02, -5.2091e-03, -2.3700e-02, -5.5951e-02,\n",
       "         -6.8752e-02, -6.7743e-02, -2.2828e-02, -5.7623e-02, -4.5998e-02,\n",
       "         -1.0067e-01, -9.4562e-02, -4.1535e-02, -6.7216e-02, -5.6506e-02,\n",
       "         -5.4841e-02, -5.8529e-02,  3.4181e-03, -5.6073e-02, -6.0273e-02,\n",
       "         -2.5481e-02, -2.7097e-03, -3.5562e-02, -3.8937e-02, -5.2278e-02,\n",
       "         -3.1474e-02, -1.2762e-03, -4.9172e-02, -3.4192e-02, -4.3727e-02,\n",
       "          1.9726e-03,  1.6661e-03, -8.2595e-02, -5.8742e-02, -2.3027e-02,\n",
       "         -5.6505e-02, -5.1417e-02, -2.7953e-02,  8.2025e-03, -2.6651e-02,\n",
       "         -8.4184e-02,  6.5978e-02, -6.5232e-02, -5.8546e-02, -4.9873e-02,\n",
       "         -1.5686e-02, -7.4861e-03, -3.1441e-02,  2.3907e-02, -6.6235e-02,\n",
       "         -4.0473e-02, -1.4005e-03, -4.0311e-02, -5.5116e-03, -3.7816e-02,\n",
       "         -5.5197e-02, -9.1835e-02,  1.3707e-02, -5.3332e-02, -1.6264e-03,\n",
       "         -1.3012e-02, -7.8072e-03, -4.9740e-02, -3.6587e-02, -6.7863e-02,\n",
       "         -3.9892e-04, -7.1846e-02,  6.3449e-03, -3.7372e-02, -1.2920e-02,\n",
       "         -1.9650e-02, -5.0132e-02, -5.8311e-02, -5.6309e-02, -1.4240e-03,\n",
       "         -2.3317e-02, -1.2338e-01, -3.5167e-02, -4.8042e-02, -2.7075e-02,\n",
       "         -1.7889e-02, -7.1499e-02, -5.3586e-02,  1.2390e-02, -5.0772e-02,\n",
       "         -5.9427e-02, -1.0049e-01, -1.8396e-02, -1.8511e-03,  1.4519e-01,\n",
       "         -4.5802e-02, -3.1196e-02, -6.1138e-02, -7.4502e-02, -5.0838e-02,\n",
       "         -3.1000e-02,  1.4570e-03, -6.5435e-02, -7.8160e-02, -5.0671e-02,\n",
       "         -1.4435e-02, -2.6770e-02, -2.8153e-02, -9.9754e-02, -3.7531e-02,\n",
       "          6.0460e-03,  6.9589e-03, -2.0935e-02,  2.7066e-02, -6.4990e-02,\n",
       "         -9.2199e-02, -8.4786e-03, -3.2897e-02, -2.3206e-02, -5.5834e-02,\n",
       "         -8.2410e-02, -3.5277e-02, -7.2363e-02, -8.7228e-03, -2.5876e-02,\n",
       "         -1.1565e-01,  3.1905e-02, -1.2073e-02, -7.3085e-02, -1.4397e-02,\n",
       "          6.5846e-03, -8.5408e-03, -4.2651e-02, -4.4961e-02,  1.5482e-03,\n",
       "         -1.9417e-02, -5.5561e-02, -1.3952e-01, -2.4405e-02, -3.3381e-02,\n",
       "         -2.9844e-02, -2.0342e-02,  1.4017e-02, -1.2988e-02, -8.2590e-02,\n",
       "          1.6130e-02, -1.9902e-02,  2.6419e-02, -8.4311e-02,  1.5315e-02,\n",
       "         -4.3580e-02, -6.4932e-02, -1.6876e-02, -2.7609e-02, -1.7167e-02,\n",
       "         -1.0359e-02, -6.2968e-02, -2.5631e-02, -5.4178e-02, -5.1445e-02,\n",
       "         -6.3412e-02, -1.8374e-02, -2.0207e-02, -3.1582e-02, -4.5678e-02,\n",
       "         -4.8130e-02,  1.0181e-02, -5.2456e-02, -2.0623e-02, -1.1742e-02,\n",
       "         -1.5048e-02, -4.5725e-02,  1.9840e-02, -1.0330e-01, -7.3163e-02,\n",
       "          2.2767e-02, -5.9509e-02, -3.1141e-02, -8.5431e-02, -1.8573e-02,\n",
       "         -1.2971e-01, -5.1127e-02, -6.2067e-02, -5.2425e-02, -3.7160e-02,\n",
       "          1.6371e-02, -8.2952e-02,  1.4366e-02, -1.9854e-02, -1.0519e-02,\n",
       "         -6.6269e-02, -1.0013e-01, -6.0413e-02, -2.8088e-02, -3.2466e-02,\n",
       "          4.2548e-02, -1.8014e-03, -4.6596e-02, -2.0161e-02, -1.9057e-02,\n",
       "         -4.7243e-02,  3.4371e-04, -5.7619e-02, -2.6253e-03, -4.0880e-03,\n",
       "         -6.4277e-02, -5.2779e-02, -5.1926e-02, -1.7635e-02, -6.4351e-02,\n",
       "         -4.2296e-02, -6.1533e-02, -5.0485e-02, -8.8422e-02, -3.8365e-03],\n",
       "        device='mps:0'),\n",
       " 'linear2.weight': tensor([[-0.0439, -0.0362, -0.0231,  ..., -0.0831,  0.0075,  0.0271],\n",
       "         [ 0.0196,  0.0212, -0.0118,  ..., -0.0479, -0.0157, -0.0179],\n",
       "         [ 0.0310, -0.0421,  0.0027,  ..., -0.0030,  0.0482,  0.0031],\n",
       "         ...,\n",
       "         [ 0.0284, -0.0382, -0.0114,  ..., -0.1470,  0.1179,  0.0168],\n",
       "         [ 0.0119, -0.0687, -0.0174,  ..., -0.0349,  0.0053, -0.0217],\n",
       "         [-0.0328,  0.0101, -0.0242,  ...,  0.0659, -0.0473,  0.0166]],\n",
       "        device='mps:0'),\n",
       " 'linear2.bias': tensor([-0.1039, -0.1240, -0.0407,  ..., -0.1498, -0.0210, -0.0416],\n",
       "        device='mps:0'),\n",
       " 'linear3.weight': tensor([[ 0.0530, -0.0522,  0.0243,  ..., -0.0352, -0.0137, -0.0227],\n",
       "         [-0.0621, -0.0764, -0.0244,  ..., -0.2777, -0.0052, -0.0292],\n",
       "         [ 0.0356, -0.0916, -0.0027,  ..., -0.2019,  0.0046,  0.0053],\n",
       "         ...,\n",
       "         [-0.0693, -0.0470, -0.0156,  ...,  0.0319, -0.0131, -0.0494],\n",
       "         [-0.0523,  0.0393, -0.0051,  ..., -0.0688, -0.0040, -0.0648],\n",
       "         [-0.0821, -0.0042, -0.0223,  ..., -0.1147, -0.0136, -0.0518]],\n",
       "        device='mps:0'),\n",
       " 'linear3.bias': tensor([ 0.0272, -0.0259, -0.0464, -0.1175, -0.0424, -0.0590, -0.0808, -0.0801,\n",
       "          0.2504, -0.0102], device='mps:0')}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:06<00:00, 164.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.968\n",
      "wrong counts for the digit 0 : 13\n",
      "wrong counts for the digit 1 : 18\n",
      "wrong counts for the digit 2 : 44\n",
      "wrong counts for the digit 3 : 22\n",
      "wrong counts for the digit 4 : 24\n",
      "wrong counts for the digit 5 : 43\n",
      "wrong counts for the digit 6 : 16\n",
      "wrong counts for the digit 7 : 32\n",
      "wrong counts for the digit 8 : 47\n",
      "wrong counts for the digit 9 : 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total  = 0\n",
    "\n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x,y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = net(x.view(-1,784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct += 1\n",
    "                else:\n",
    "                    wrong_counts[y[idx]] += 1\n",
    "                total += 1\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i} : {wrong_counts[i]}')\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W torch.Size([1000, 784]) + B: torch.Size([1000])\n",
      "Layer 2: W torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
      "Layer 3: W torch.Size([10, 2000]) + B: torch.Size([10])\n",
      "Total number of parameters: 2807010\n"
     ]
    }
   ],
   "source": [
    "total_parameters_original = 0\n",
    "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
    "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(f'Layer {index + 1}: W {layer.weight.shape} + B: {layer.bias.shape}')\n",
    "print(f'Total number of parameters: {total_parameters_original}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in,rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0,std=1)\n",
    "\n",
    "\n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "def linear_layer_parameterization(layer, device, rank=1, lora_aplha=1):\n",
    "\n",
    "    features_in, features_out = layer.weight.shape\n",
    "    \n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out, rank = rank, alpha=lora_aplha, device = device\n",
    "    )\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.linear1, 'weight', linear_layer_parameterization(net.linear1, device)\n",
    ")\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    net.linear2, 'weight', linear_layer_parameterization(net.linear2, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    net.linear3, 'weight', linear_layer_parameterization(net.linear3, device)\n",
    ")\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [net.linear1,net.linear2,net.linear3]:\n",
    "        layer.parametrizations['weight'][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B \" torch.Size([1000]) + Lora_A : torch.Size([1, 784]) + Lora_B : torch.Size([1000, 1])\n",
      "Layer 2: W: torch.Size([2000, 1000]) + B \" torch.Size([2000]) + Lora_A : torch.Size([1, 1000]) + Lora_B : torch.Size([2000, 1])\n",
      "Layer 3: W: torch.Size([10, 2000]) + B \" torch.Size([10]) + Lora_A : torch.Size([1, 2000]) + Lora_B : torch.Size([10, 1])\n",
      "Total number of params (original): 2807010\n",
      "Total number of params (original + LoRA): 2813804\n",
      "Params introduced by LoRA: 6794\n",
      "Params Increment :  0.242\n"
     ]
    }
   ],
   "source": [
    "total_parameters_lora = 0 \n",
    "total_parameters_non_lora = 0 \n",
    "for index, layer in enumerate([net.linear1,net.linear2,net.linear3]):\n",
    "    total_parameters_lora += layer.parametrizations['weight'][0].lora_A.nelement() + layer.parametrizations['weight'][0].lora_B.nelement()\n",
    "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(\n",
    "        f'Layer {index +1 }: W: {layer.weight.shape} + B \" {layer.bias.shape} + Lora_A : {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B : {layer.parametrizations[\"weight\"][0].lora_B.shape}')\n",
    "    \n",
    "\n",
    "print(f'Total number of params (original): {total_parameters_non_lora}')\n",
    "print(f'Total number of params (original + LoRA): {total_parameters_non_lora + total_parameters_lora}')\n",
    "print(f'Params introduced by LoRA: {total_parameters_lora}')\n",
    "params_increment = (total_parameters_lora/ total_parameters_non_lora) * 100\n",
    "print(f'Params Increment :  {params_increment:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear1.bias',\n",
       " 'linear1.parametrizations.weight.original',\n",
       " 'linear1.parametrizations.weight.0.lora_A',\n",
       " 'linear1.parametrizations.weight.0.lora_B',\n",
       " 'linear2.bias',\n",
       " 'linear2.parametrizations.weight.original',\n",
       " 'linear2.parametrizations.weight.0.lora_A',\n",
       " 'linear2.parametrizations.weight.0.lora_B',\n",
       " 'linear3.bias',\n",
       " 'linear3.parametrizations.weight.original',\n",
       " 'linear3.parametrizations.weight.0.lora_A',\n",
       " 'linear3.parametrizations.weight.0.lora_B']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[0] for i in net.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the Non-Lora params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear1.bias\n",
      "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear2.bias\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear3.bias\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/595 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  60%|█████▉    | 595/1000 [00:07<00:05, 80.67it/s, loss=0.000315]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True,download=True,transform = transform)\n",
    "exclude_indices = mnist_trainset.targets == 9\n",
    "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "train(train_loader,net,epochs=1, total_iterations_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that forzen params are still unchanged \n",
    "\n",
    "assert torch.all(net.linear1.parametrizations.weight.orginal == original_weights['linear1.weight'])\n",
    "assert torch.all(net.linear2.parametrizations.weight.orginal == original_weights['linear2.weight'])\n",
    "assert torch.all(net.linear3.parametrizations.weight.orginal == original_weights['linear3.weight'])\n",
    "\n",
    "\n",
    "enable_disable_lora(enabled=True)\n",
    "# The new linear1.weight is obtained by the 'forward' function of our LoRA paramtrization\n",
    "# The original weights have been moved to net.linear1.parametrization.weight.original \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:10<00:00, 92.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.968\n",
      "wrong counts for the digit 0 : 13\n",
      "wrong counts for the digit 1 : 18\n",
      "wrong counts for the digit 2 : 44\n",
      "wrong counts for the digit 3 : 22\n",
      "wrong counts for the digit 4 : 24\n",
      "wrong counts for the digit 5 : 43\n",
      "wrong counts for the digit 6 : 16\n",
      "wrong counts for the digit 7 : 32\n",
      "wrong counts for the digit 8 : 47\n",
      "wrong counts for the digit 9 : 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 1000/1000 [00:15<00:00, 63.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.322\n",
      "wrong counts for the digit 0 : 583\n",
      "wrong counts for the digit 1 : 917\n",
      "wrong counts for the digit 2 : 634\n",
      "wrong counts for the digit 3 : 771\n",
      "wrong counts for the digit 4 : 918\n",
      "wrong counts for the digit 5 : 825\n",
      "wrong counts for the digit 6 : 166\n",
      "wrong counts for the digit 7 : 992\n",
      "wrong counts for the digit 8 : 973\n",
      "wrong counts for the digit 9 : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
